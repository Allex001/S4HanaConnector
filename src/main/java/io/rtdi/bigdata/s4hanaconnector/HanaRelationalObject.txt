package io.rtdi.bigdata.s4hanaconnector;

import java.sql.CallableStatement;
import java.sql.JDBCType;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.avro.Schema;

import com.fasterxml.jackson.annotation.JsonIgnore;

import io.rtdi.bigdata.connector.connectorframework.exceptions.ConnectorRuntimeException;
import io.rtdi.bigdata.connector.connectorframework.rest.entity.BrowsingNode;
import io.rtdi.bigdata.connector.connectorframework.rest.entity.BrowsingNode.Condition;
import io.rtdi.bigdata.connector.pipeline.foundation.SchemaConstants;
import io.rtdi.bigdata.connector.pipeline.foundation.avro.JexlGenericData.JexlRecord;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroBoolean;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroBytes;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroCLOB;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroDate;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroDecimal;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroDouble;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroFloat;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroInt;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroLong;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroNCLOB;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroNVarchar;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroSTGeometry;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroSTPoint;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroShort;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroTime;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroTimestamp;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroTimestampMicros;
import io.rtdi.bigdata.connector.pipeline.foundation.avrodatatypes.AvroVarchar;
import io.rtdi.bigdata.connector.pipeline.foundation.exceptions.SchemaException;
import io.rtdi.bigdata.connector.pipeline.foundation.recordbuilders.AvroField;
import io.rtdi.bigdata.connector.pipeline.foundation.recordbuilders.AvroRecordArray;
import io.rtdi.bigdata.connector.pipeline.foundation.recordbuilders.SchemaBuilder;
import io.rtdi.bigdata.connector.pipeline.foundation.utils.NameEncoder;

public class HanaRelationalObject extends RelationalObject<HanaRelationalObject> {

	protected RelationalObject<?> root;

	public HanaRelationalObject() {
		super();
	}

	@Override
	@JsonIgnore
	protected RelationalObject<?> getRoot() {
		return root;
	}

	public HanaRelationalObject(String mastertable, String alias, HanaRelationalObject parent) throws ConnectorRuntimeException {
		super(mastertable, alias);
		setParent(parent);
		root = parent.getRoot();
		// addColumns();
	}

	public HanaRelationalObject(String mastertable, String alias) throws ConnectorRuntimeException {
		super(mastertable, alias);
	}
	
	public void addAllColumns() throws ConnectorRuntimeException {
		addColumns();
		if (getChildren() != null) {
			for (HanaRelationalObject c : getChildren()) {
				c.addAllColumns();
			}
		}
	}
	
	@Override
	public void setChildren(List<HanaRelationalObject> children) {
		super.setChildren(children);
		if (children != null) {
			for (HanaRelationalObject r : children) {
				r.root = getRoot();
			}
		}
	}

	public void addColumns() throws ConnectorRuntimeException {
		String sql = "select c.column_name, c.data_type_name, c.length, c.scale, p.position " + 
				"from table_columns c left outer join constraints p " + 
				"	on (p.is_primary_key = 'TRUE' and p.schema_name = c.schema_name and p.table_name = c.table_name and p.column_name = c.column_name) " + 
				"where c.schema_name = ? and c.table_name = ? " + 
				"order by c.position";
		try (PreparedStatement stmt = root.getConn().prepareStatement(sql);) {
			stmt.setString(1, root.getSourcedbschema());
			stmt.setString(2, getMastertable());
			ResultSet rs = stmt.executeQuery();
			while (rs.next()) {
				ColumnMapping m = addMapping(rs.getString(1), "\"" + getAlias() + "\".\"" + rs.getString(1) + "\"", getHanaDataType(rs.getString(2), rs.getInt(3), rs.getInt(4)));
				if (rs.getInt(5) != 0) {
					addPK(rs.getInt(5), m);
				}
			}
		} catch (SQLException e) {
			throw new ConnectorRuntimeException("Reading the table definition failed", e, 
					"Execute the sql as Hana user \"" + root.getUsername() + "\"", sql);
		}
	}
	
	@Override
	protected HanaRelationalObject createNewRelationalObject(String detailtable, String alias) throws ConnectorRuntimeException {
		return new HanaRelationalObject(detailtable, alias, this);
	}
	
	private static String getHanaDataType(String datatype, int length, int scale) {
		switch (datatype) {
		case "DECIMAL": // decimal(p, s) with p between 1..38 and scale 0..p
			return datatype + "(" + length + ", " + scale + ")";
		case "CHAR":
		case "VARCHAR":
		case "VARBINARY":
		case "NCHAR":
		case "NVARCHAR":
			return datatype + "(" + length + ")";
		default:
			return datatype;
		}
	}

	public void createTriggers() throws ConnectorRuntimeException {
		createTrigger();
		if (getChildren() != null) {
			for (HanaRelationalObject r : getChildren()) {
				r.createTriggers();
			}
		}
	}
	
	
	
	private void createTrigger() throws ConnectorRuntimeException {
		String sql = "select right(trigger_name, 1) from triggers " + 
				"where subject_table_schema = ? and subject_table_name = ? and trigger_name like subject_table_name || '\\_t\\__' escape '\\'";
		try (PreparedStatement stmt = root.getConn().prepareStatement(sql);) {
			stmt.setString(1, root.getSourcedbschema());
			stmt.setString(2, getMastertable());
			ResultSet rs = stmt.executeQuery();
			String existingtriggers = "";
			while (rs.next()) {
				existingtriggers += rs.getString(1);
			}
			if (existingtriggers.length() < 3) {
				StringBuffer pklist1 = new StringBuffer();
				StringBuffer pklist2 = new StringBuffer();
				for (int i = 0; i < getPKColumns().size(); i++) {
					String pkcolumnname = getPKColumns().get(i);
					if (pkcolumnname == null) {
						throw new ConnectorRuntimeException("The table is not using all primary key columns", null, 
								"Make sure all pk columns are mapped at least", getMastertable() + ": " + getPKColumns().toString());
					}
					if (i != 0) {
						pklist1.append(',');
						pklist2.append(',');
					}
					pklist1.append(":c.\"");
					pklist1.append(pkcolumnname);
					pklist1.append('"');
					pklist2.append("PK");
					pklist2.append(i+1);
				}
				if (getPKColumns().size() == 0) {
					throw new ConnectorRuntimeException("This replication technology does only work on tables with primary keys", null, 
							"Please remove the table specified from the list of tables to be replicated", getMastertable());
				} else if (getPKColumns().size() > 6) {
					throw new ConnectorRuntimeException("Currently only tables with up the six primary keys are allowed", null, 
							"Please create and issue", getMastertable() + ": " + pklist1.toString());
				} else {
					if (!existingtriggers.contains("i")) {
						sql = "CREATE TRIGGER \"" + getMastertable() + "_t_i\" " + 
								" AFTER INSERT ON \"" + root.getSourcedbschema() + "\".\"" + getMastertable() + "\" " + 
								" REFERENCING NEW ROW c " + 
								" FOR EACH ROW " + 
								" BEGIN " + 
								"     INSERT INTO \"" + root.getUsername() + "\".PKLOG "
										+ "(change_ts, schema_name, table_name, change_type, "
										+ "transactionid, transaction_seq, "
										+ pklist2.toString() + ") " + 
								"     VALUES (now(), '" + root.getSourcedbschema() + "', '" + getMastertable() + "', 'I', "
										+ "CURRENT_UPDATE_TRANSACTION(), CURRENT_UPDATE_STATEMENT_SEQUENCE(), "
										+ pklist1.toString() + " ); " + 
								" END"; 
						try (PreparedStatement stmttr = root.getConn().prepareStatement(sql);) {
							stmttr.execute();
						}
					}
					if (!existingtriggers.contains("u")) {
						sql = "CREATE TRIGGER \"" + getMastertable() + "_t_u\" " + 
								" AFTER UPDATE ON \"" + root.getSourcedbschema() + "\".\"" + getMastertable() + "\" " + 
								" REFERENCING NEW ROW c " + 
								" FOR EACH ROW " + 
								" BEGIN " + 
								"     INSERT INTO \"" + root.getUsername() + "\".PKLOG "
										+ "(change_ts, schema_name, table_name, change_type, "
										+ "transactionid, transaction_seq, "
										+ pklist2.toString() + ") " + 
								"     VALUES (now(), '" + root.getSourcedbschema() + "', '" + getMastertable() + "', 'U', "
										+ "CURRENT_UPDATE_TRANSACTION(), CURRENT_UPDATE_STATEMENT_SEQUENCE(), "
										+ pklist1.toString() + " ); " + 
								" END"; 
						try (PreparedStatement stmttr = root.getConn().prepareStatement(sql);) {
							stmttr.execute();
						}
					}
					if (!existingtriggers.contains("d")) {
						sql = "CREATE TRIGGER \"" + getMastertable() + "_t_d\" " + 
								" BEFORE DELETE ON \"" + root.getSourcedbschema() + "\".\"" + getMastertable() + "\" " + 
								" REFERENCING OLD ROW c " + 
								" FOR EACH ROW " + 
								" BEGIN " + 
								"     INSERT INTO \"" + root.getUsername() + "\".PKLOG "
										+ "(change_ts, schema_name, table_name, change_type, "
										+ "transactionid, transaction_seq, "
										+ pklist2.toString() + ") " + 
								"     VALUES (now(), '" + root.getSourcedbschema() + "', '" + getMastertable() + "', 'D', "
										+ "CURRENT_UPDATE_TRANSACTION(), CURRENT_UPDATE_STATEMENT_SEQUENCE(), "
										+ pklist1.toString() + " ); " + 
								" END"; 
						try (PreparedStatement stmttr = root.getConn().prepareStatement(sql);) {
							stmttr.execute();
						}
					}
				}

			}
		} catch (SQLException e) {
			throw new ConnectorRuntimeException("Creating the Change Logging triggers failed in the database", e, 
					"Execute the sql as Hana user \"" + root.getUsername() + "\"", sql);
		}
	}

	public void createTypes() throws ConnectorRuntimeException {
		createType();
		if (getChildren() != null) {
			for (HanaRelationalObject r : getChildren()) {
				r.createTypes();
			}
		}
	}
	
	private void createType() throws ConnectorRuntimeException {
		String mastertable = getRoot().getMastertable();
		/*
		 * As the table type has to match the procedure perfectly, it is better to drop and recreate those.
		 */
		try {
			String sql = "drop type \"" + mastertable + "_" + getAlias() + "\"";
			CallableStatement callable = root.getConn().prepareCall(sql);
			callable.execute();
		} catch (SQLException e) {
			// ignore types that do not exist
		}
		StringBuffer typebuffer = new StringBuffer();
		typebuffer.append("create type \"");
		typebuffer.append( mastertable + "_" + getAlias());
		typebuffer.append("\" as table (");
		
		if (getRoot() == this) {
			typebuffer.append("_path nvarchar(5000), _change_type varchar(1), _transactionid bigint, ");
		} else {
			typebuffer.append("_path nvarchar(5000), _parent_path nvarchar(5000), ");
		}
		boolean first = true;
		for (ColumnMapping m : getColumnmappings()) {
			if (first) {
				first = false;
			} else {
				typebuffer.append(", ");
			}
			typebuffer.append("\"");
			typebuffer.append(m.getAlias());
			typebuffer.append("\" ");
			typebuffer.append(m.getHanadatatype());
		}
		typebuffer.append(")");
		
		try (CallableStatement callable = root.getConn().prepareCall(typebuffer.toString());) {
			callable.execute();
		} catch (SQLException e) {
			throw new ConnectorRuntimeException("Creating the table type failed", e, 
					"Execute the sql as Hana user \"" + root.getUsername() + "\"", typebuffer.toString());
		}
	}

	protected void generateOutputParameters(String tablename, StringBuffer outparams) {
		/*
		 * outparams should be like: out L1 rootname_L1, out L2 rootname_L2
		 * where L1, L2 are the aliases and rootname_L1, rootname_L2 are the table type names
		 */
		if (outparams.length() != 0) {
			outparams.append(", ");
		}
		outparams.append("out ");
		outparams.append(getAlias());
		outparams.append(" \"");
		outparams.append(tablename);
		outparams.append("_");
		outparams.append(getAlias());
		outparams.append("\"");
		if (getChildren() != null) {
			for (HanaRelationalObject r : getChildren()) {
				r.generateOutputParameters(tablename, outparams);
			}
		}
	}

	private void generateCallParameters(StringBuffer outparams) {
		if (outparams.length() != 0) {
			outparams.append(", ");
		}
		outparams.append("?");
		if (getChildren() != null) {
			for (HanaRelationalObject r : getChildren()) {
				r.generateCallParameters(outparams);
			}
		}
	}

	
	private String prodecurecallstring = null;
	
	public CallableStatement prepareDeltaProcedureCall() throws SQLException {
		if (prodecurecallstring == null) {
			StringBuffer outparams = new StringBuffer();
			generateCallParameters(outparams);
			String params = outparams.toString();
			prodecurecallstring = "{CALL \"" + getMastertable() + "_CHANGES\"(?,?," + params + ")}";
		}
		return root.getConn().prepareCall(prodecurecallstring);
	}
	
	public List<JexlRecord> fetchResult(long min_transactionid, long max_transactionid) throws SQLException, ConnectorRuntimeException, SchemaException {
		List<JexlRecord> result = new ArrayList<>();
		Map<String, JexlRecord> index = new HashMap<>();
		try (CallableStatement stmt = prepareDeltaProcedureCall();) {
			stmt.setLong(1, min_transactionid);
			stmt.setLong(2, max_transactionid);
			boolean moreResults = stmt.execute();
			if (moreResults) {
				fetchResultSet(stmt, index, result);
			}
		}
		return result;
	}
	
	private void fetchResultSet(CallableStatement stmt, Map<String, JexlRecord> index, List<JexlRecord> result) throws SQLException, ConnectorRuntimeException, SchemaException {
	    try (ResultSet rs = stmt.getResultSet();) {
	    	while (rs.next()) {
	    		JexlRecord r = convert(rs);
	    		index.put(rs.getString(1), r); // Param 1 --> _path
	    		if (getParent() != null) {
	    			String parentpath = rs.getString(2);
	    			JexlRecord parentrecord = index.get(parentpath);
	    			if (parentrecord != null) {
	    				String parentfield = getParentfieldname();
	    				@SuppressWarnings("unchecked")
						List<JexlRecord> a = (List<JexlRecord>) parentrecord.get(parentfield);
	    				if (a == null) {
	    					a = new ArrayList<>();
	    					parentrecord.put(parentfield, a);
	    				}
	    				a.add(r);
	    			}
	    		} else {
	    			r.put(SchemaConstants.SCHEMA_COLUMN_CHANGE_TYPE, rs.getString(2));
	    			r.put(SchemaConstants.SCHEMA_COLUMN_SOURCE_TRANSACTION, rs.getString(3));
	    			result.add(r);
	    		}
	    	}
	    }
	    stmt.getMoreResults();
		if (getChildren() != null) {
			for (HanaRelationalObject r : getChildren()) {
				r.fetchResultSet(stmt, index, result);
			}
		}
	}

	private JexlRecord convert(ResultSet rs) throws SQLException, ConnectorRuntimeException, SchemaException {
		JexlRecord r = new JexlRecord(getAvroSchema());
		int startindex;
		if (getParent() == null) {
			startindex = 4;
		} else {
			startindex = 3;
		}
		for (int i=startindex; i<= rs.getMetaData().getColumnCount(); i++) {
			String columnname = rs.getMetaData().getColumnName(i);
			if (!fkcolumns.contains(columnname)) {
				String avrofieldname = NameEncoder.encodeName(columnname);
				int datatype = rs.getMetaData().getColumnType(i);
				JDBCType t = JDBCType.valueOf(datatype);
				switch (t) {
				case BIGINT:
					r.put(avrofieldname, rs.getLong(i));
					break;
				case BINARY:
					r.put(avrofieldname, rs.getBytes(i));
					break;
				case BLOB:
					r.put(avrofieldname, rs.getBytes(i));
					break;
				case BOOLEAN:
					r.put(avrofieldname, rs.getBoolean(i));
					break;
				case CHAR:
					r.put(avrofieldname, rs.getString(i));
					break;
				case CLOB:
					r.put(avrofieldname, rs.getString(i));
					break;
				case DATE:
					r.put(avrofieldname, rs.getDate(i));
					break;
				case DECIMAL:
					r.put(avrofieldname, rs.getBigDecimal(i));
					break;
				case DOUBLE:
					r.put(avrofieldname, rs.getDouble(i));
					break;
				case FLOAT:
					r.put(avrofieldname, rs.getFloat(i));
					break;
				case INTEGER:
					r.put(avrofieldname, rs.getInt(i));
					break;
				case LONGNVARCHAR:
					r.put(avrofieldname, rs.getString(i));
					break;
				case LONGVARBINARY:
					r.put(avrofieldname, rs.getBytes(i));
					break;
				case LONGVARCHAR:
					r.put(avrofieldname, rs.getString(i));
					break;
				case NCHAR:
					r.put(avrofieldname, rs.getString(i));
					break;
				case NCLOB:
					r.put(avrofieldname, rs.getString(i));
					break;
				case NVARCHAR:
					r.put(avrofieldname, rs.getString(i));
					break;
				case REAL:
					r.put(avrofieldname, rs.getFloat(i));
					break;
				case SMALLINT:
					r.put(avrofieldname, rs.getInt(i));
					break;
				case TIME:
					r.put(avrofieldname, rs.getTime(i));
					break;
				case TIMESTAMP:
					r.put(avrofieldname, rs.getTimestamp(i));
					break;
				case TIMESTAMP_WITH_TIMEZONE:
					r.put(avrofieldname, rs.getTimestamp(i));
					break;
				case TIME_WITH_TIMEZONE:
					r.put(avrofieldname, rs.getTimestamp(i));
					break;
				case TINYINT:
					r.put(avrofieldname, rs.getByte(i));
					break;
				case VARBINARY:
					r.put(avrofieldname, rs.getBytes(i));
					break;
				case VARCHAR:
					r.put(avrofieldname, rs.getString(i));
					break;
				default:
					throw new ConnectorRuntimeException("The select statement returns a datatype the connector cannot handle", null, 
							"Please create an issue", rs.getMetaData().getColumnName(i) + ":" + t.getName());
				}
			}
		}
		return r;
	}
	
	protected void addChangeSQL(StringBuffer sql) {
		if (getParent() == null) { // root is simple, as the pklog contains the pks directly for sure
			sql.append("select ");
			for (int i = 0; i < getPKColumns().size(); i++) {
				sql.append("PK");
				sql.append(i+1);
				sql.append(" as \"");
				sql.append(getPKColumns().get(i));
				sql.append("\", ");
			}
			sql.append("transactionid as _transactionid from pklog where table_name = '");
			sql.append(getMastertable());
			sql.append("' and schema_name = '");
			sql.append(root.getSourcedbschema());
			sql.append("'\r\n");
		}
	}
	
	/**
	 * Starting with the most detailed record
	 * 
	 * @param detail as the table with the most detailed information 
	 * @param sourcedbschema 
	 * @return the select statement needed to get all root PKs that changed due to this detail table change.
	 */
	private static StringBuffer createSelect(HanaRelationalObject detail, String sourcedbschema) {
		/*
		 * Example:
		 * L1 -> L2 -> L3 -> L4 -> L5 --> select L1.pk from PKLOG join L4 join L3 join L2 join L1
		 */
		
		/*
		 * First, find the top most table containing all root PKs.
		 * Case 1: L1, L2 and L3 all have the L1's PK columns based on the join condition. L1.PK = L2.FK = L3.FK --> No need to join down to L1 if L3 has all the information already.
		 * Case 2: L4 contains the L1.PK column --> The join between L4 and PKLOG is sufficient
		 * Case 3: L5 contains the L1.PK column but it is not its primary key --> Read PKLOG to get the changed L5 rows, L5 has the L1.PK
		 * Case 4: L5 contains the L1.PK and it is its primary key --> PKLOG is enough, no joins.
		 * 
		 * Therefore, the first thing to do is building the entire path from root to detail
		 */
		List<RelationalObject<HanaRelationalObject>> path = new ArrayList<>();
		RelationalObject<HanaRelationalObject> r = detail;
		while (r != null) {
			path.add(0, r); // push the most recent element
			r = r.getParent();
		}
		HanaRelationalObject root = detail.getRoot();
		/*
		 * For each detail table list the transitive master.PK column names. 
		 * Example:
		 *   master.PK={MANDT, VBELN}
		 *   join condition for detail1: left.MANDT = right.MANDT; left.VBELN = right.VBELN
		 *   join condition for detail2: left.ID = right.ID
		 * Hence treeresult = [ [ MANDT, VBELN ], [ MANDT , VBELN ], [      ] ]
		 *                        master             detail1          detail2
		 * 
		 * So detail2 needs to be joined with detail1 in order to get the master PK values but does not have to join to the master
		 * 
		 */
		List<List<String>> treeresult = new ArrayList<>();
		List<String> pklist_root = new ArrayList<>();
		treeresult.add(pklist_root);
		for (String pkcolumn : root.getPKColumns()) { // L1.PK
			pklist_root.add(pkcolumn);
			String recentcolumnname = pkcolumn;
			for (int i=1; i<path.size(); i++) {
				RelationalObject<HanaRelationalObject> join = path.get(i); // L1.PK = L2.FK
				List<String> columns;
				if (treeresult.size() <= i) {
					columns = new ArrayList<>();
					treeresult.add(columns);
				} else {
					columns = treeresult.get(i);
				}
				for (Condition c : join.getJoincondition()) {
					if (c.getLeft().equals(recentcolumnname)) {
						columns.add(c.getRight());
						recentcolumnname = c.getRight();
						break;
					}
				}
			}
		}
		int i;
		/*
		 * starting from the master, find the highest detail table containing all root.PK columns.
		 */
		for (i=treeresult.size()-1; i>=0; i--) {
			if (treeresult.get(i).size() >= root.getPKColumns().size()) {
				break;
			}
		}

		RelationalObject<HanaRelationalObject> mintable;
		int joincase;
		List<String> fkcolumns = treeresult.get(i);
		mintable = path.get(i);
		if (path.size()-1 == i && detail.getPKColumns().containsAll(fkcolumns)) {
			// Case 4: order.PK = item.FK and item.FK is part of PK --> PKLOG is sufficient
			joincase = 4;
		} else if (!detail.getPKColumns().containsAll(fkcolumns)) {
			// Case 3: order.ORDERID = item.NOTEID and item.NOTEID is not part of the PK --> PKLOG joined with item
			joincase = 3;
		} else {
			// TODO: Distinct the remaining cases
			// Case 1
			// Case 2: order.ORDERID = item.ORDERITEM and item.ORDERITEM is part of the FK
			joincase = 2;
		}
		
		if (joincase == 4) {
			// PKLOG contains all data needed
			StringBuffer sql = new StringBuffer();
			sql.append("select ");
			for (String fk : fkcolumns) {
				sql.append("PK");
				int pos = detail.getPKColumns().indexOf(fk);
				sql.append(pos+1);
				sql.append(" as \"");
				sql.append(fk);
				sql.append("\", ");
			}
			sql.append("transactionid as _transactionid from pklog where table_name = '");
			sql.append(detail.getMastertable());
			sql.append("' and schema_name = '");
			sql.append(sourcedbschema);
			sql.append("'\r\n");
			return sql;
		} else if (joincase == 3) {
			StringBuffer from = new StringBuffer();
			from.append("from pklog ");
			from.append("join \"");
			from.append(sourcedbschema);
			from.append("\".\"");
			from.append(detail.getMastertable());
			from.append("\" as \"");
			from.append(detail.getAlias());
			from.append("\" on (");
			for (int j=0; j<detail.getPKColumns().size(); j++) {
				if (j != 0) {
					from.append(" and ");
				}
				from.append("pklog.PK");
				from.append(j+1);
				from.append(" = \"");
				from.append(detail.getAlias());
				from.append("\".\"");
				from.append(detail.getPKColumns().get(j));
				from.append("\"");
			}
			from.append(") ");
			/*
			 * Add the join with L3 and subsequent L2 and L1
			 */
			addParentJoin(from, detail, sourcedbschema, mintable);
			StringBuffer sql = new StringBuffer();
			sql.append("select ");
			for (int j = 0; j < root.getPKColumns().size(); j++) {
				sql.append("\"");
				sql.append(mintable.getAlias());
				sql.append("\".\"");
				sql.append(fkcolumns.get(j));
				sql.append("\" as \"");
				sql.append(fkcolumns.get(j));
				sql.append("\", ");
			}
			sql.append("pklog.transactionid as _transactionid ");
			sql.append(from);
			sql.append(" where pklog.table_name = '");
			sql.append(detail.getMastertable());
			sql.append("' and pklog.schema_name = '");
			sql.append(sourcedbschema);
			sql.append("'\r\n");
			return sql;
		} else {
			/* 
			 * Case 1,2
			 * 
			 * FROM clause: 
			 * L5.PK == PKLOG.PK, hence L5 does not need to be used and can be replaced with PKLOG.
			 * For that reason the join between L5 and L4 is slightly different, it is PKLOG with L4. Hence this is handled separately here.
			 */
			RelationalObject<HanaRelationalObject> master = detail.getParent();
			StringBuffer from = new StringBuffer();
			from.append("from pklog ");
			from.append("join \"");
			from.append(sourcedbschema);
			from.append("\".\"");
			from.append(master.getMastertable());
			from.append("\" as \"");
			from.append(master.getAlias());
			from.append("\" on (");
			boolean first = true;
			for (BrowsingNode.Condition condition : detail.getJoincondition()) {
				if (first) {
					first = false;
				} else {
					from.append(" and ");
				}
				int pos = detail.getPKColumns().indexOf(condition.getLeft());
				from.append("pklog.PK");
				from.append(pos+1);
				from.append(" = \"");
				from.append(master.getAlias());
				from.append("\".\"");
				from.append(condition.getLeft());
				from.append("\"");
			}
			from.append(") ");
			/*
			 * Add the join with L3 and subsequent L2 and L1
			 */
			addParentJoin(from, master, sourcedbschema, mintable);
			StringBuffer sql = new StringBuffer();
			sql.append("select ");
			for (int j = 0; j < root.getPKColumns().size(); j++) {
				sql.append("\"");
				sql.append(mintable.getAlias());
				sql.append("\".\"");
				sql.append(fkcolumns.get(j));
				sql.append("\" as \"");
				sql.append(fkcolumns.get(j));
				sql.append("\", ");
			}
			sql.append("pklog.transactionid as _transactionid ");
			sql.append(from);
			sql.append(" where pklog.table_name = '");
			sql.append(detail.getMastertable());
			sql.append("' and pklog.schema_name = '");
			sql.append(sourcedbschema);
			sql.append("'\r\n");
			return sql;
		}
	}

	private static void addParentJoin(StringBuffer from, RelationalObject<HanaRelationalObject> detail, String sourcedbschema, RelationalObject<HanaRelationalObject> mintable) {
		if (detail != null && detail != mintable) { // if the table had all columns already, there is no need to join the master for the PKs
			RelationalObject<HanaRelationalObject> master = detail.getParent();
			if (master != null) {
				from.append("join \"");
				from.append(sourcedbschema);
				from.append("\".\"");
				from.append(master.getMastertable());
				from.append("\" as \"");
				from.append(master.getAlias());
				from.append("\" on (");
				boolean first = true;
				for (Condition condition : detail.getJoincondition()) {
					if (first) {
						first = false;
					} else {
						from.append(" and ");
					}
					from.append("\"");
					from.append(detail.getAlias());
					from.append("\".\"");
					from.append(condition.getRight());
					from.append("\" = \"");
					from.append(master.getAlias());
					from.append("\".\"");
					from.append(condition.getLeft());
					from.append("\"");
				}
				from.append(") ");
				addParentJoin(from, master, sourcedbschema, mintable);
			}
		}
	}
	
	protected Schema avroschema = null;
	
	@JsonIgnore
	public Schema getAvroSchema() throws SchemaException, ConnectorRuntimeException {
		return avroschema;
	}

	protected void createSchema(SchemaBuilder valueschema) throws ConnectorRuntimeException {
		try {
			for ( ColumnMapping m : getColumnmappings()) {
				String hanadatatypestring = m.getHanadatatype();
				String columnname = m.getAlias();
				if (! fkcolumns.contains(columnname)) { // The detail schema should not contain the parent join columns as they would be redundant 
					AvroField f = valueschema.add(columnname, getDataType(hanadatatypestring), null, true);
					if (getParent() == null && getPKColumns().contains(m.getAlias())) {
						f.setPrimaryKey();
					}
				}
			}
			avroschema = valueschema.getSchema();
			if (getChildren() != null) {
				for (HanaRelationalObject r : getChildren()) {
					AvroRecordArray f = valueschema.addColumnRecordArray(r.getParentfieldname(), null, r.getMastertable(), null);
					SchemaBuilder childschema = f.getArrayElementSchemaBuilder();
					r.createSchema(childschema);
				}
			}
		} catch (SchemaException e) {
			throw new ConnectorRuntimeException("The Avro Schema cannot be created due to an internal error", e, 
					"Please create an issue", valueschema.toString());
		}
	}

	public static Schema getDataType(String datatypestring) throws ConnectorRuntimeException {
		Pattern p = Pattern.compile("(\\w*)\\s*\\(?\\s*(\\d*)\\s*\\,?\\s*(\\d*)\\s*\\)?.*");  // decimal(10,3) plus spaces anywhere, three groups
		Matcher m = p.matcher(datatypestring);
		m.matches();
		String datatype = m.group(1);
		String lengthstring = m.group(2);
		int length = 0;
		int scale = 0;
		if (lengthstring != null && lengthstring.length() != 0) {
			length = Integer.valueOf(lengthstring);
			String scalestring = m.group(3);
			if (scalestring != null && scalestring.length() != 0) {
				scale = Integer.valueOf(scalestring);
			}
		}
		switch (datatype) {
		case "TINYINT": // 8-bit unsigned(!!) integer; 0..255
			return AvroShort.getSchema();
		case "SMALLINT": // 16-bit signed integer; -32,768..32,767
			return AvroShort.getSchema();
		case "INTEGER": // 32-bit signed integer; -2,147,483,648..2,147,483,647
			return AvroInt.getSchema();
		case "BIGINT": // 64-bit signed integer
			return AvroLong.getSchema();
		case "DECIMAL": // decimal(p, s) with p between 1..38 and scale 0..p
			return AvroDecimal.getSchema(length, scale);
		case "REAL": // 32-bit floating-point number
			return AvroFloat.getSchema();
		case "DOUBLE": // 64-bit floating-point number
			return AvroDouble.getSchema();
		case "SMALLDECIMAL": // The precision and scale can vary within the range 1~16 for precision and -369~368 for scale
			return AvroDecimal.getSchema(length, scale);
		case "CHAR":
		case "VARCHAR": // 7-bit ASCII string
			return AvroVarchar.getSchema(length);
		case "BINARY":
			return AvroBytes.getSchema();
		case "VARBINARY":
			return AvroBytes.getSchema();
		case "DATE":
			return AvroDate.getSchema();
		case "TIME":
			return AvroTime.getSchema();
		case "TIMESTAMP":
			return AvroTimestampMicros.getSchema();
		case "CLOB":
			return AvroCLOB.getSchema();
		case "BLOB":
			return AvroBytes.getSchema();
		case "NCHAR":
			return AvroNVarchar.getSchema(length);
		case "NVARCHAR":
			return AvroNVarchar.getSchema(length);
		case "ALPHANUM":
			return AvroVarchar.getSchema(length);
		case "NCLOB":
			return AvroNCLOB.getSchema();
		case "TEXT":
			return AvroNCLOB.getSchema();
		case "BINTEXT":
			return AvroBytes.getSchema();
		case "SHORTTEXT":
			return AvroNCLOB.getSchema();
		case "SECONDDATE":
			return AvroTimestamp.getSchema();
		case "ST_POINT":
			return AvroSTPoint.getSchema();
		case "ST_GEOMETRY":
			return AvroSTGeometry.getSchema();
		case "BOOLEAN":
			return AvroBoolean.getSchema();
		default:
			throw new ConnectorRuntimeException("Table contains a data type which is not known", null, "Newer Hana version??", datatype);
		}
	}

}
